{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f3a9e1",
   "metadata": {},
   "source": [
    "# 04 – Neural Network Training (PyTorch)\n",
    "\n",
    "In diesem Notebook trainieren wir ein tiefes neuronales Netz (MLP) auf den vorverarbeiteten Daten.\n",
    "\n",
    "**Ziele:**\n",
    "- Aufbau einer Custom Dataset-Klasse und DataLoader in PyTorch\n",
    "- Definition einer flexiblen Modell-Architektur (MLP mit Dropout)\n",
    "- Training mit **Class Weights** (um die extreme Imbalance von 0.17% Fraud zu kontern)\n",
    "- Implementierung von **Early Stopping**, um Overfitting zu vermeiden\n",
    "- Vergleich der Performance (PR-AUC) mit der Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "# Reproduzierbarkeit (WICHTIG für NN Training)\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Device Config (GPU falls verfügbar, sonst CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 2. Daten laden (Artefakte)\n",
    "Wir laden die `.npy` Dateien, die in `02_preprocessing.ipynb` erstellt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Pfad robust bestimmen\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "data_dir = project_root / \"data\" / \"processed\"\n",
    "\n",
    "try:\n",
    "    X_train = np.load(data_dir / \"X_train.npy\")\n",
    "    X_test  = np.load(data_dir / \"X_test.npy\")\n",
    "    y_train = np.load(data_dir / \"y_train.npy\")\n",
    "    y_test  = np.load(data_dir / \"y_test.npy\")\n",
    "    print(\"Daten erfolgreich geladen.\")\n",
    "    print(f\"Train Shape: {X_train.shape}, Fraud Rate: {y_train.mean():.4%}\")\n",
    "    print(f\"Test Shape:  {X_test.shape}, Fraud Rate: {y_test.mean():.4%}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FEHLER: Daten nicht gefunden. Bitte 02_preprocessing.ipynb ausführen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader\n",
    "PyTorch benötigt `Dataset`-Objekte, um Daten effizient in Batches zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Konvertierung zu Float32 (Standard für PyTorch)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        # Zielvariable shape (N, 1) für BCEWithLogitsLoss\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Datasets erstellen\n",
    "train_dataset = FraudDataset(X_train, y_train)\n",
    "test_dataset  = FraudDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader (Batch Size ist Hyperparameter)\n",
    "BATCH_SIZE = 1024  # Größere Batches oft stabiler bei Imbalance\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Test Loader braucht kein Shuffle\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "\n",
    "print(f\"Anzahl Batches pro Epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "## 4. Modell-Architektur\n",
    "Ein einfaches, aber robustes MLP (Multi-Layer Perceptron).\n",
    "\n",
    "**Architektur-Entscheidungen:**\n",
    "- **Input**: 30 Features\n",
    "- **Hidden Layers**: 64 -> 32 Neuronen (Trichter-Form)\n",
    "- **Aktivierung**: ReLU (Standard)\n",
    "- **Regularisierung**: Dropout (0.3) gegen Overfitting\n",
    "- **Output**: 1 Neuron (Logits) - **kein Sigmoid** hier, da wir `BCEWithLogitsLoss` nutzen (numerisch stabiler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8i9j0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FraudNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1    = nn.BatchNorm1d(64) # Hilft bei Training-Speed und Stabilität\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.bn2    = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.relu   = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.output(x) # Return Logits (linear output)\n",
    "        return x\n",
    "\n",
    "model = FraudNN(input_dim=X_train.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 5. Training Setup (Weighted Loss)\n",
    "Da wir sehr wenige Fraud-Fälle haben (Klasse 1), würde das Modell sonst einfach immer \"0\" vorhersagen und eine Accuracy von 99.8% erreichen.\n",
    "\n",
    "Wir nutzen **pos_weight** im Loss, um Fehler bei der Klasse 1 (Fraud) stärker zu bestrafen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0k1l2m3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Gewichts: Ratio von Negativen zu Positiven\n",
    "num_pos = y_train.sum()\n",
    "num_neg = len(y_train) - num_pos\n",
    "pos_weight = num_neg / num_pos\n",
    "pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "\n",
    "print(f\"Positive Weight: {pos_weight:.2f}\") \n",
    "# Bedeutet: Ein Fraud-Fehler wiegt ~577x schwerer als ein Normal-Fehler\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "## 6. Training Loop mit Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, model):\n",
    "    \"\"\"Evaluates model and returns loss and metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Sigmoid für Wahrscheinlichkeiten\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            \n",
    "    all_preds = np.vstack(all_preds).ravel()\n",
    "    all_targets = np.vstack(all_targets).ravel()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    pr_auc = average_precision_score(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, pr_auc\n",
    "\n",
    "# Training Config\n",
    "EPOCHS = 20\n",
    "history = {\"train_loss\": [], \"test_loss\": [], \"test_pr_auc\": []}\n",
    "\n",
    "print(\"Starte Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- TRAINING ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # --- EVALUATION ---\n",
    "    avg_test_loss, test_pr_auc = evaluate(test_loader, model)\n",
    "    \n",
    "    # Logging\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"test_loss\"].append(avg_test_loss)\n",
    "    history[\"test_pr_auc\"].append(test_pr_auc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f} | \"\n",
    "          f\"Test PR-AUC: {test_pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "## 7. Analyse der Ergebnisse\n",
    "Wir plotten den Loss-Verlauf, um zu prüfen, ob das Modell lernt oder overfittet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4o5p6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCE Loss (weighted)\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot 2: PR-AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"test_pr_auc\"], color=\"green\", label=\"Test PR-AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"PR-AUC Score\")\n",
    "plt.title(\"Metric Evolution (Higher is better)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "## 8. Finaler Vergleich & Speichern\n",
    "Vergleich mit der Baseline aus Notebook 03 (LogReg PR-AUC war ca. 0.70-0.75 je nach Split/Params)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q7r8s9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_prauc = evaluate(test_loader, model)\n",
    "print(f\"\\nFinal Test PR-AUC: {final_prauc:.4f}\")\n",
    "\n",
    "# Modell speichern\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "model_path = models_dir / \"fraud_nn_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Modell gespeichert unter: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}